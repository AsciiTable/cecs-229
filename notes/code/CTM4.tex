\documentclass [12pt]{article}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\topmargin}{-.7in}
\setlength{\textheight}{9.25in}
\setlength{\textwidth}{6.5in}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{framed}
\usepackage{epsfig}
\usepackage{changebar}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{stmaryrd}
\graphicspath{ {images/} }
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\usepackage{multicol}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\setlength{\columnsep}{1cm}
% mathematical commands
\newcommand{\zos}{{\{ 0,1\}^{\ast}}}
\newcommand{\zoi}{{\{ 0,1\}^{\infty}}}
\newcommand{\zon}{{\{ 0,1\}}}
\newcommand{\zov}[1]{{\{ 0,1\}^{#1}}}
\newcommand{\ccc}{{{\cal C}}}
\newcommand{\gggg}{{{\cal G}}}
\newcommand{\nat}{{{\cal N}}}
\newcommand{\rr}{{{\bf RAND}}}
\newcommand{\pref}{{\sqsubset}}
\newcommand{\da}{{\downarrow}}
\newcommand{\ot}{{\otimes}}
\newcommand{\fann}{{\forall n\in \nat}}
\newcommand{\pow}{{{\cal P}}}
\newcommand{\nll}{{{\bf NULL}}}
\newcommand{\nvc}[1]{{{\bf e_{#1}}}}
\newcommand{\st}{{\Sigma_{2}^{A}}}
\newcommand{\ov}[1]{{\overline{#1}}}
\newcommand{\provided}{{\hspace{.1in}:-\hspace{.1in}}}
\begin{document}
\begin{center}\title*{\Large \S \; 4 Basis I \& II}\\\author*{Jessica Wei} \end{center}
\normalsize
\noindent\textbf{MOTIVATION}: Earlier we saw ${[1,0],[0,1]}$ generates $\mathbb{R}^2$ i.e.
\[Span{[1,0],[0,1]}=\mathbb{R}^2\]
What does this mean? For any vector $\overrightarrow{v}=[a,b]\in\mathbb{R}^2$, we can define its location as 
\[a[1,0]+b[0,1]=\overrightarrow{v}\]
\begin{framed}
\noindent\textbf{DEF} $|$ Coordinates\\
Let $G$ be a generating set of vectors and $V$ be a vector space over a field $\mathbb{F}$
\[Span(G)=V\]
The coordinates of a vector $\overrightarrow{v}\in V$ with respect to the set $G$ are the coefficients $\alpha_1,\alpha_2,...,\alpha_n$ where 
\[\overrightarrow{v}=\alpha_1\overrightarrow{g}_1+\alpha_2\overrightarrow{g}_2+...+\alpha_n\overrightarrow{g}_n\]
and $\overrightarrow{g}_i\in G$.
\end{framed} 
\noindent\textbf{Example. }Find the coordinates of vector $\overrightarrow{v}=[1,2]$ with respect to the given generating set of $\mathbb{R}^2$.
\begin{enumerate}[\quad(a)]
    \item $G={[1,1],[-1,1]}$\\
    $[1,2]=\alpha[1,1],\beta[-1,1]$\\
    $[\alpha,\alpha]+[-\beta,\beta]$\\
    $[\alpha-\beta,\alpha+\beta]$\\
    $[1,2]=\frac{3}{2}[1,1]+\frac{1}{2}[-1,1]$\\
    Coordinates: $<\frac{3}{2},\frac{1}{2}>G$
    \item $G_2={[-2,0],[0,1]}$\\
    $[1,2]=\alpha[-2,0]+\beta[0,1]$\\
    $[-2\alpha,0]+[0,\beta]$\\
    $-2\alpha = 1, \beta=2$\\
    $\alpha = \frac{-1}{2}$\\
    Coordinates: $<\frac{-1}{2}, 2>_{G_2}$
\end{enumerate}
NOTE: We could also argue that $H={[1,0],[0,1],[2,0]}$ also generates $\mathbb{R}^2$ e.g. $\overrightarrow{v}\in\mathbb{R}$
\[\overrightarrow{v}=\alpha[1,0]+\beta[0,1]+\lambda[2,0]\]
\[1,2=[\alpha,0]+[0,\beta]+[2\lambda,0] = [\alpha+2\lambda,\beta]\]
\[\beta=1\indent\alpha=-1\indent\lambda=1\indent<-1,1,1>_H\]
\[\beta=1\indent\alpha=1\indent\lambda=0\indent<1,1,0>_H\]
\[\beta=1\indent\alpha=2\indent\lambda=-\frac{1}{2}\indent<1,2,-\frac{1}{2}>_H\]
We obtain more than one possible set of coordinates for the same vector under the same generating set. Confusing!
\pagebreak
\begin{framed}
\noindent\textbf{DEF} $|$ Linear Dependence\\
The vectors ${\overrightarrow{v_1}, \overrightarrow{v_2},...,\overrightarrow{v_k}}$ are linearly dependent if we can find scalars $\alpha_1,\alpha_2,...,\alpha_k\in\mathbb{F}$ not all zero such that
\[\alpha_1\overrightarrow{v}_1+\alpha_2\overrightarrow{v}_2+...+\alpha_k\overrightarrow{v}_k=\overrightarrow{0}\]
\[\Rightarrow\alpha_i\overrightarrow{v}_i =-\alpha_1\overrightarrow{v}_1-\alpha_2\overrightarrow{v}_2-...-\alpha_k\overrightarrow{v}_k\]
\[\Rightarrow\overrightarrow{v}_i =\frac{\alpha_1}{\alpha_i}\overrightarrow{v}_1-\frac{\alpha_2}{\alpha_i}\overrightarrow{v}_2-...-\frac{\alpha_k}{\alpha_i}\overrightarrow{v}_k\]
i.e. any vector in the set can be written as a linear combination of the rest of the vectors.
\end{framed}
\noindent\textbf{Example. }Determine if the set of vectors is linearly dependent.
\begin{enumerate}[\quad(a)]
    \item ${[1,0],[2,0]}$ \\
    $\alpha_1[1,0]+\alpha_2[2,0] = [0,0]$\\
    $alpha_1=-2,\alpha_2=1\Rightarrow$ linearly dependent
    \item {[1,0,0],[0,2,0],[2,4,0],[0,1,0]}\\
    $\alpha_1[1,0,0]+\alpha_2[0,2,0]+\alpha_3[2, 4,0]+\alpha_4[0,1,0] = \overrightarrow{0}$\\
    $\alpha_1=-2\indent\alpha_2=-2\indent\alpha_3=1\indent\alpha_4=0\Rightarrow$ linearly dependent
    \item ${[1,0,0],[0,2,0],[0,0,4]}$\\
    $\alpha_1[1,0,0]+\alpha_2[0,2,0]+\alpha_3[0,0,4]=[0,0,0]$\\
    $\alpha_1=0\indent\alpha_2=0\indent\alpha_3=0\Rightarrow$ linearly independent
\end{enumerate}
\begin{framed}
\noindent\textbf{LEMMA 1}\\
Let $G={\overrightarrow{v}_1,...,\overrightarrow{v}_k}$ where $\overrightarrow{v}_i\in\mathbb{F}^2$. Then $\overrightarrow{x}\in Span(G)$ if and only if $\exists\alpha_i\in\mathbb{F}$ such that 
\[\alpha_1\overrightarrow{v}_1+\alpha_2\overrightarrow{v}_2+...+\alpha_k\overrightarrow{v}_k+\alpha_{k+1}\overrightarrow{x}=0\]
and not all scalars are 0.
\end{framed}
\noindent\textbf{Proof. }Assume
\[\alpha_1\overrightarrow{v}_1+...+\alpha_k\overrightarrow{v}_k+\alpha_{k+1}\overrightarrow{x}=0\]
and not all scalars are 0. Then,
\[\alpha_{k+1}\overrightarrow{x}=-\alpha_1\overrightarrow{v}_1-...-\alpha_k\overrightarrow{v}_k\]
\[\overrightarrow{x}=\frac{-\alpha_1}{\alpha_{k+1}}\overrightarrow{v}_1-...-\frac{-\alpha_k}{\alpha_{k+1}}\overrightarrow{v}_k\]
\[=\beta_1\overrightarrow{v}_1+...+\beta_k\overrightarrow{v}_k\]
where $\beta_i=\frac{-\alpha_i}{\alpha_{k+1}}$\\
$\Rightarrow\overrightarrow{x}\in Span(G)$\\
Now assume $\overrightarrow{x}tSpan(G)$. Then $\exists\lambda_i\in\mathbb{F}$ such that 
\[\overrightarrow{x}=\lambda_1\overrightarrow{v}_1+...+\lambda_k\overrightarrow{v}_k\]
\[\Rightarrow\lambda_1\overrightarrow{v}_1+...+\lambda_k\overrightarrow{v}_k+(-1)\overrightarrow{x}=\overrightarrow{0}\]
Hence $\lambda_1\overrightarrow{v}_1+...+\lambda_k\overrightarrow{v}_k+\lambda_{k+1}\overrightarrow{x}=\overrightarrow{0}$ where $\lambda_{k+1}=-1$
\pagebreak
\begin{framed}
\noindent\textbf{LEMMA 2}\\
Let $S={\overrightarrow{v}_1,\overrightarrow{v_2,...,\overrightarrow{v}_k,\overrightarrow{x}}}$ such that $\overrightarrow{x}=\alpha_1\overrightarrow{v}+...+\alpha_k\overrightarrow{v}_k$. Then $Span(S)=Span(S-{\overrightarrow{x}})$
\end{framed}
\noindent\textbf{Proof} NTS 1. $Span(S)\leq Span(S-{\overrightarrow{x}})$ and 2. $Span(S-{\overrightarrow{x}})\leq Span S$
\begin{enumerate}[\quad]
    \item(1) Pick an arbitrary vector $\overrightarrow{u}\in Span(S)$. Then\\
    $\overrightarrow{u}=\beta_1\overrightarrow{v}_1+...+\beta_k\overrightarrow{v}_k+\beta_{k+1}\overrightarrow{x}$\\
    $=\beta_1\overrightarrow{v}_1+...+\beta_k\overrightarrow{v}_k+\beta_{k+1}(\alpha_1\overrightarrow{v}_1+...+\alpha_k\overrightarrow{v}_k)$\\
    $=\beta_1\overrightarrow{v}_1+...+\beta_k\overrightarrow{v}_k+\beta_{k+1}\alpha_1\overrightarrow{v}_1+...+\beta_{k+1}\alpha_k\overrightarrow{v}_k$\\
    $=(\beta_1+\beta_{k+1}\alpha_1)\overrightarrow{v}_1+...+(\beta_k+\beta_{k+1}\alpha_k)\overrightarrow{v}_k$\\
    $=\lambda_1\overrightarrow{v}_1+...+\lambda_k\overrightarrow{v}_k$\\
    $\Rightarrow\overrightarrow{u}\in Span(S-{\overrightarrow{x}})$\\
    $\Rightarrow Span(S)\leq Span(S-{\overrightarrow{x}})$
    \item To show (2), pick an arbitrary $\overrightarrow{w}\in Span(S-{\overrightarrow{x}})$. Then,\\
    $\overrightarrow{w}=w_1\overrightarrow{v}_1+...+w_k\overrightarrow{v}_k=w_1\overrightarrow{v}_1+...+w_k\overrightarrow{v}_k+0\overrightarrow{x}$\\
    $\Rightarrow\overrightarrow{w}\in Span(S)$\\
    $Span(S-{\overrightarrow{x}})\leq Span(S)$\\
    By (1) and (2) it follows that $Span$ $S=Span(S-{\overrightarrow{x}})$
\end{enumerate}
\begin{framed}
\noindent\textbf{DEF} $|$ Basis\\
Let $G$ be a set of vectors. We call $G$ a basis for a vector space $V$ if
\begin{enumerate}[(i)]
    \item $Span(G)=V$ \indent i.e. $G$ generates $V$
    \item $G$ is a linearly independent set
\end{enumerate}
e.g $G={[1,0],[0,1]}$ is a basis for $\mathbb{R}^2$\\
$T={[1,0],[0,1],[0,2]}$ is a generating set for $\mathbb{R}^2$ i.e. $R^2=Span(T)$ but $T$ is NOT a basis.
\end{framed}
\pagebreak
\noindent\textbf{Size of a Basis}
\begin{framed}
\noindent\textbf{LEMMA 3} Let $V$ be a vector space, $S$ be a generating set such that $Span(S)=V$ and $B$ be a subset $V$ that is linearly independent. Then
\[|S|\geq |B|\]
size of a generating set $\geq$ size of a linearly independent set 
\end{framed}
\begin{framed}
\noindent\textbf{THM} Basis Theorem\\
Let $V$ be a vector space. All bases for $V$ have the same size.
\end{framed}
\noindent\textbf{Proof.} Suppose $B_1$ and $B_2$ are bases for $V$.\\
*$B_1$ is a bases means (i) $Span(B_1)=V$ and (ii) $B_1$ is linearly independent.\\
*$B_2$ is a basis means (iii) $Span(B_2)=V$ and (iv) $B_2$ is linearly independent.\\
From Lemma 3.\\
(i) \& (iv) $|B_2|\leq|B_1|$ assuming $B_2\leq V$ (which is true b/c any vector in $B_2$ *)\\
(ii) \& (iii) $|B_1|=|B_2|\Rightarrow $ bases have the same size.
\begin{framed}
\noindent\textbf{THM} Let $V$ be a vector space, $B$ a set of generators for $V$. $B$ will be the smallest set of generators if and only if it is the basis for $V$.\\
Assume $B$ is a basis for $V$. Consider some set of generators $T$. Since $B$ is a basis, it is linearly independent \& by Lemma 3
\[|B|\leq|T|\]
Since $T$ is arbitrary, the result holds for all sets of generators. Hence, B is the smallest generating set.\\
Now, assume $B$ is the smallest generating set but is not a basis. i.e. either\\
$Span(B)\neq V\indent\leftarrow\indent$ FALSE\\
or $B$ is not linearly independent $\leftarrow$ TRUE\\
If $B$ is not linearly independent, there is some vector $\overrightarrow{x}\in B$ that is a linear combination of the other vectors in $B$. By Lemma 2, then,
\[V=Span(B)=Span(B-{\overrightarrow{x}})\]
i.e. $B$ is NOT the smallest generating set which is a contradiction. So $B$ MUST be a basis.
\end{framed}
\begin{framed}
\noindent\textbf{DEF} $|$ Dimension\\
The dimension of a vector space is the size of a basis for $V=dim(V)=|B|$ where $B$ is a basis.
\end{framed}
\pagebreak
\noindent\textbf{Example.} Find the dimension of each set.
\begin{enumerate}[\quad a)]
    \item $\mathbb{R}^3$\\
    $B=\begin{bmatrix}
    [1,0,0]^T\\
    [0,1,0]^T\\
    [0,0,1]^T
    \end{bmatrix}$\\ is a basis because (i) B is linearly independent and (ii) $Span(B)={[\alpha,\beta,\lambda]^T}=\mathbb{R}^3$\\
    $dim(\mathbb{R}^3)=|B|=3$
    \item $\mathbb{R}^2$\\
    $B={[1,0]^T,[0,1]^T}$ is a basis; $|B|=2$\\
    $dim(\mathbb{R}^2=|B|=2$
\end{enumerate}
\begin{framed}
\noindent\textbf{DEF} $|$ Rank\\
Let $A\in\mathbb{F}_{m\times n}$. The Row Rank of A, $row(A)$ or $rowRank(A)$, is the dimension of the row space of A. i.e. $row(A)=dim(rowsp(A_c))$\\
Find a basis for the rowspace, then find the size of that basis. Similarly $colRank(A)=dim(colsp(A))$
\end{framed}
\noindent\textbf{Example.} 
Let $A=\begin{bmatrix}
1&0&0\\
0&2&0\\
2&4&0
\end{bmatrix}$ Find the ranks of $A$.\\\\
*$rowsp(A)={[1,0,0],[0,2,0],[2,4,0]}$\\
$B_{row}={[1,0,0],[0,2,0]}$ linearly indep. \& $Span(B_{row})=rowsp(B)$\\
$rowRank(A)=|B_{row}|=2$\\\\
*$colsp(A)={[1,0,2]^T,[0,2,4]^T,[0,0,0]^T}$\\
$B_{col}={[1,0,2]^T,[0,2,4]^T}$\\
$colRank(A)=|B_{col}|=2$
\begin{framed}
\noindent\textbf{THM}\\
Let $A\in\mathbb{F}_{n\times n}.$ $rowRank(A)=colRank(A)$. We refer to any rank of $A$ simply as $rank(A)$.
\end{framed}
\noindent\textbf{Q.} Given a vector space $V$, how do we find a basis for $V$?
\begin{framed}
\noindent\textbf{A1. }Shrink Algorithm
\begin{lstlisting}
def shrink(V):
    B = V
    for v in B:
        if Span(B - v) == V:
            B = B - v
    return B
\end{lstlisting}
\end{framed}
\pagebreak
\begin{framed}
\noindent\textbf{A2. }Growth Algorithm
\begin{lstlisting}
def grow(V):
    B = null
    for v in V:
        if v not in Span(B): #i.e. if v is linearly independent
            B = B union {v}  #add v to B
    return B
\end{lstlisting}
\end{framed}
\noindent\textbf{Proof.} Show that the grow algorithm always returns a basis for $V$.\\
NTS (i) B is linearly independent and (ii) $Span(B)=V$\\
To show i) it suffices to consider the condition for the if statement if $\overrightarrow{v}\notin Span(B)$. This shows that a linear combination of vectors already in B. Hence, if none of the vectors in B are linear combinations of each other, they are linearly independent. \\\\
\noindent\textbf{Example. }Find a basis for $v={[4,1,2]^T,[0,0,1]^T,[5,4,3]^T,[2,0,1]^T}$ using the growth algorithm: $B = \emptyset$\\
Check: $[4,1,2]\in Span(\emptyset)\rightarrow $ False\\
\indent $B=\{[4,1,2]\}$\\
Check: $[0,0,1]\in Span([4,1,2])\rightarrow$ False\\
\indent $B=\{[4,1,2],[0,0,1]\}$\\
Check: $[5,4,3]\in Span(\{[4,1,2],[0,0,1]\})$\\
\indent $\alpha[4,1,2]+\beta[0,0,1]=[5,4,3]$\\
\indent $[4\alpha,\alpha,2\alpha+\beta]=[5,4,3]$\\
\indent $\alpha_1=\frac{5}{3}$ and $\alpha_2=4\rightarrow$ Contradiction!\\
$B=\{[4,1,2],[0,0,1],[5,4,3]\}$\\\\
\noindent\textbf{Check.} $[2,0,1]\in Span(B) \Rightarrow$ True\\
$\alpha\begin{bmatrix}
4\\
1\\
2
\end{bmatrix}+
\beta\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}+
\lambda\begin{bmatrix}
5\\
4\\
3
\end{bmatrix}=
\begin{bmatrix}
4\alpha+5\lambda\\
1\alpha+4\lambda\\
2\alpha+\beta+3\lambda
\end{bmatrix}=
\begin{bmatrix}
2\\
0\\
1
\end{bmatrix}$\\\\
Returns $B=\{[4,1,2],[0,0,1],[5,4,3]\}$
\\\\\\
\noindent\textbf{Q. }How do we program the grow algorithm?\\
Let's revisit the problem $V=\begin{bmatrix}
\begin{bmatrix}
4\\
1\\
2\\
\overrightarrow{v_1}
\end{bmatrix}, \begin{bmatrix}
0\\
0\\
1\\
\overrightarrow{v_2}
\end{bmatrix},\begin{bmatrix}
5\\
4\\
3\\
\overrightarrow{v_3}
\end{bmatrix},\begin{bmatrix}
2\\
0\\
1\\
\overrightarrow{v_4}
\end{bmatrix}\end{bmatrix}$\\\\
ITER 1: $B=\{\overrightarrow{v_1}\}$\\
ITER 2: Is there a scalar $\alpha$ such that $\alpha\overrightarrow{v}_1=\overrightarrow{v}_2$?\\\\
$\alpha\begin{bmatrix}
4\\
1\\
2
\end{bmatrix}=\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}$\indent If no, add $\overrightarrow{v}_2$ into $B$. $B={\overrightarrow{v}_1,\overrightarrow{v}_2}$\\
ITER 3: Are there scalars $\alpha_1\&\alpha_2$ such that $\alpha_1\overrightarrow{v}_1+\alpha_2\overrightarrow{v}_2=\overrightarrow{v}_3$?\\
$\alpha_2\overrightarrow{v}_2=\overrightarrow{v}_3?$\\
$\alpha_1\begin{bmatrix}
4\\
1\\
2
\end{bmatrix}+\alpha_2\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}=\begin{bmatrix}
5\\
4\\
3
\end{bmatrix}\Rightarrow B=\{\overrightarrow{v}_1,\overrightarrow{v}_2,\overrightarrow{v}_3\}$\\\\
ITER 4: Are there $\alpha_1,\alpha_2,\alpha_3$ such that $\alpha_1\overrightarrow{v}_1+\alpha_2\overrightarrow{v}_2+\alpha_3\overrightarrow{v}_3=\overrightarrow{v}_4?$\\
$\alpha_1\begin{bmatrix}
4\\
1\\
2
\end{bmatrix}+\alpha_2\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}+\alpha_3\begin{bmatrix}
5\\
4\\
3
\end{bmatrix}=\begin{bmatrix}
2\\
0\\
1
\end{bmatrix} = ?$\\\\
$\begin{bmatrix}
4&0&5\\
1&0&4\\
2&1&3
\end{bmatrix}\begin{bmatrix}
\alpha_1\\
\alpha_2\\
\alpha_3
\end{bmatrix}=\begin{bmatrix}
2\\
0\\
1
\end{bmatrix}$\\\\
$\begin{bmatrix}
4\alpha_1+0\alpha_2+5\alpha_3\\
\alpha_1+0\alpha_2+4\alpha_3\\
2\alpha_1+\alpha_2+3\alpha_3
\end{bmatrix}$\indent Notice: we are solving a problem that looks like the following: 
\[A\overrightarrow{x}=\overrightarrow{b}\]
Where $\overrightarrow{A}$ consists of column vectors of $B$, $\overrightarrow{x}$ is a vector of unknown scalars, and $\overrightarrow{v}$ is the vector that we are checking for condition $\overrightarrow{v}\notin Span(B)$.\\\\
\noindent\textbf{Solving $A\overrightarrow{x}=\overrightarrow{b}$}
\begin{framed}
\noindent\textbf{DEF} $|$ Upper Triangular\\
A matrix $A\in\mathbb{F}_{m\times n}$ is called upper triangular if all elements below the main diagonal are zero. \\
i.e. $A=\begin{bmatrix}
1&2&3\\
0&4&5\\
0&0&6
\end{bmatrix}$
\end{framed}
\begin{framed}
\noindent\textbf{DEF} $|$ Lower Triangular\\
A matrix is lower triangular if every entry above the main diagonal is zero.\\
i.e. $A=\begin{bmatrix}
1&0&0\\
2&6&0\\
3&4&7
\end{bmatrix}$\\
If $A$ is upper or lower triangular then $A\overrightarrow{x}=\overrightarrow{b}$ can be solved by Forward or Backward substitution.
\end{framed}
\noindent\textbf{Example. }Solve $A\overrightarrow{x}=\overrightarrow{b}$ where $A=\begin{bmatrix}
1&2&3\\
0&4&5\\
0&0&6
\end{bmatrix}$ and $\overrightarrow{b}=\begin{bmatrix}
8\\
11\\
18
\end{bmatrix}$\\\\
$A\overrightarrow{x}=\overrightarrow{b}=\begin{bmatrix}
1&2&3\\
0&4&5\\
0&0&6
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}=\begin{bmatrix}
8\\
11\\
18
\end{bmatrix}$\\
\pagebreak

\noindent$x_1+2x_2+3x_3=8$\\
$4x_2+5x_3=11$\\
$6x_3=18\Rightarrow x_3=3$\\
*From (3), $x_3=3$. Substitute $x_3=3$ into (2)\\
$4x_2+15=11$\\
$4x_2=-4$\\
$x_2=-1$\\
*Substitute $x_3=3, x_2=-1$ into (1)\\
$x_1-2+9=8$
$x_1+7=8$
$x_1=1$\\\\

\noindent\textbf{Solving $A\overrightarrow{x}=\overrightarrow{b}$ if $A$ is not upper or lower triangular}\\
\textbf{Option 1.} Use the inverse matrix
\begin{framed}
\noindent\textbf{DEF} $|$ Inverse Matrix\\
Let $A\in\mathbb{F}_{n\times n}$, the inverse matrix of $A$, $A^{-1}$, is the square matrix satisfying
\[AA^{-1}=A^{-1}A=I_m\]
$I_m$ being the identity matrix with 1's along diagonal and 0's everywhere else.\\
i.e. $A=\begin{bmatrix}
1&1\\
3&4
\end{bmatrix}$ has inverse $\begin{bmatrix}
4&-1\\
-3&1
\end{bmatrix}$\\\\
$AA^{-1}=\begin{bmatrix}
1&1\\
3&4
\end{bmatrix}\begin{bmatrix}
4&-1\\
-3&1
\end{bmatrix}=\begin{bmatrix}
4-3&-1+1\\
12-12&-3+4
\end{bmatrix}=\begin{bmatrix}
1&0\\
0&1
\end{bmatrix}$\\\\
If $A$ has an inverse $A^{-1}$, then
\[A\overrightarrow{x}=b^{-1}\rightarrow A^{-1}A\overrightarrow{x}=A^{-1}\overrightarrow{b}\rightarrow I_m\overrightarrow{x}=A^{-1}\overrightarrow{b}\rightarrow\overrightarrow{x}=A^{-1}b^{-1}\]
$$\begin{bmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}=\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4
\end{bmatrix}$$\\
Identity Matrix $\times$ vector = same vector\\
*If $A\in\mathbb{R}_{m\times m}$ and $m\neq n$ then it does not have an inverse i.e. $A$ is a singular matrix\\
*Any $A_{mxn}$ will have an inverse if and only if the rank is $n$.
\end{framed}
\noindent\textbf{Example. }Verify that $A=\begin{bmatrix}
1&1\\
3&4
\end{bmatrix}$ has an inverse then find $A^-$. Since $\{[1,1],[3,4]\}$ are linearly independent, they form a basis for the row space $(A)$. Hence, $rank(A)=2$. Since $A$ is a $2\times2$ matrix and $rank(A)=2$, it is invertible.\\
Want: $\begin{bmatrix}
1&1\\
3&4
\end{bmatrix}\begin{bmatrix}
a&b\\
c&d
\end{bmatrix}=\begin{bmatrix}
1&0\\
0&1
\end{bmatrix}$\\\\
\pagebreak
System \#1: $a+c=1$, Column 1: $3a+1c=0$\\
$-3a-3c=-3$\\
$3a+4c=0\Rightarrow c=-3$\\
$a+(-3)=1\Rightarrow a=4$\\
System \#2: $b+d=0$, $3b+4d=1$\\
$b=-d$\\
$-3d+4d=1\Rightarrow d=1, b=-2$\\\\
\noindent\textbf{Example. }Use the inverse to solve $A\overrightarrow{x}=\overrightarrow{b}$ for $\overrightarrow{b}=\begin{bmatrix}
-1\\
-2
\end{bmatrix}$. Use the inverse from the previous example to solve.\\
ANS: From previous example\\
$A^{-1}=\begin{bmatrix}
4&-1\\
-3&1
\end{bmatrix}$\\
So $\overrightarrow{x}=A^{-1}\overrightarrow{b}\Rightarrow x=\begin{bmatrix}
4&-1\\
-3&1
\end{bmatrix}\begin{bmatrix}
-1\\
-2
\end{bmatrix}=\begin{bmatrix}
-4+2\\
3-2
\end{bmatrix}=\begin{bmatrix}
-2\\
1
\end{bmatrix}\Rightarrow\overrightarrow{x}=\begin{bmatrix}
-2\\
1
\end{bmatrix}$\\\\
\noindent\textbf{Check. }$\begin{bmatrix}
1&1\\
3&4
\end{bmatrix}\begin{bmatrix}
-2\\
1
\end{bmatrix}=\begin{bmatrix}
-1\\
-2
\end{bmatrix}$\\
$\begin{bmatrix}
-2+1\\
-6+4
\end{bmatrix}=\begin{bmatrix}
-1\\
-2
\end{bmatrix} \checkmark$\\\\\\\\
\noindent\textbf{For Grow Algorithm, we need to solve $A\overrightarrow{x}=\overrightarrow{b}$}\\
But $A$ is formed from column vectors of $\{\overrightarrow{v}_1...\overrightarrow{v}_k\}$ where $\overrightarrow{v}_i\in\mathbb{R}^n$.\\
Then $A=\begin{bmatrix}
1&1&...&1\\
\overrightarrow{v}_1&\overrightarrow{v}_2&...&\overrightarrow{v}_k\\
1&1&...&1
\end{bmatrix}_{n\times k}$
i.e. $A$ is not always a square matrix. In fact, its dimensions change at every iteration because we add one more column. Hence, $A$ will not be invertible, and we will not be able to solve $A\overrightarrow{x}=\overrightarrow{v}$ through this method of finding an inverse. Gaussian Elimination offers us hope.
\end{document}